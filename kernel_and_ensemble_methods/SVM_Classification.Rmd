---
title: "SVM Classification"
author: "Aditi Chaudhari and Abigail Solomon"
output: pdf_document

---

##SVM Classification
Generally, in Classification technique, input data is labeled in accordance with the historical data samples, and then manually trained to identify the class of the new given data.In SVM Classification, given the transactions made by the credit cards as predictors, support vectors will represent the coordinate representation of individual observation, that we utilize for segregating two classes, whether the credit card transaction is fraudulent or not. 

##Load necessary libraries
```{r}
library(e1071)

```


##Import the Data set

###Source of the Data Set

Credit Card Transactions data set: ['creditcard'dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)

###Read data
We use the read.csv() function to read a csv. Using the dim() structure function, We will see that the data set contain transactions made by credit cards, and it has 284807 observations of 31 variables along with their names.
```{r}

#Read data
df <- read.csv("creditcard.csv")
dim(df)
names(df)

```

##Data Preprocessing
Data processing is converting train raw data sets into meaningful sets that are usable.We will be examining the specific types and steps of data cleaning and later the scaling before analyzing the data. 

###Data cleaning
In order to make our data set for machine learning more meaningful, we may need to fix or remove missing or unwanted, data instances which may not help to solve the problem, from the data set. The sapply() function gives the number of missing values in each column, in this case we don't have any NA's.
```{r}

sapply(df, function(x) sum(is.na(x)==TRUE))

```
###Data Sampling
Large data sets  consume a lot of memory and took ages to run, in our case we are going to take only the first 20,000 observations,but  first we will use the unique function that repetitions will be removed, still 283726 is a big data. Since we don't have NA 's, we will randomly remove the rows after the 20,000th.

```{r}
df <- unique(df)
dim(df)
names(df)
```

```{r}
new_df <- df[-c(20001:283726), ]
str(new_df)

```

##Data Split
We are going to split our data set into training, validation and tests sets. 60% of our data will be attributed to the train data set, 20% will be attributed to the validation data, and the rest 20% will be attributed to the test data. We will then find the dimensions of the data frame using the dim() function.

```{r}

set.seed(1234)
groups <- c(train=.6, test=.2, validate=.2)
i <- sample(cut(1:nrow(new_df),
nrow(new_df)*cumsum(c(0,groups)), labels=names(groups)))
train <- new_df[i=="train",]
test <- new_df[i=="test",]
vald <- new_df[i=="validate",]
dim(new_df)

```

##Data Exploration
Data exploration helps us to gain insight into the raw train data and findings of R built-in functions.We will print the first and last six rows of transactions  using the head() and tail() functions respectively.The summary () function applied on the Amount vector, calculates summary statistics for each of them, it prints the Minimum value, the 1st quartile's value (25th percentile), the median value, the 3rd quartile's value (75th percentile) and the maximum value. 

###The first six transactions
```{r}

head(train)

```

###The last six transactions
```{r}

tail(train)

```

###Summary of the total Amount
```{r}

summary(train[c('Amount')])

```

##Visual Data Exploration
Data visualization present train data contents in graphical or picture format, enables us to grasp and understand analytics in an easier manner and be able to communicate what has been learned about the data to others, it is also optically entertaining.The Amount and the Class of the data set are plotted using Histogram, Scatter plot and Kernel density plot.

###Histogram
The Histogram graph displays the frequency of the x values, in our case it shows that small amount is spent most of the time, which is similar to the mean(Amount), which is 71.69 The plot graph similarly displays that small amount, less than 2000 are the dominant ones, the black spots show the fraud ones,tiny amount, from the findings of the Class summary, the fraud are only 0.004% of all the transaction.
```{r, warning=FALSE}

#copy original settings
opar <- par() 
#set up 1x2 grid
par(mfrow=c(1,2))

hist(train$Amount, col="cadetblue", main="Total Amount Spent", xlab="Amount")

plot(train$Amount, col=c("cadetblue","black")[train$Class + 1], main="Amount (Black are the Frauds)")

par(opar) # restore parameter settings

```

###Scatter Plot
In scatter plots, we can see the Amount versus Class similar to the results of the summary of the Class, small transactions are fraud, but the majority not.It also shows that the small amount, less than 2000 are crowded, many in number.
```{r}

plot(train$Amount, train$Class, pch='+', cex=1.5, col="cadetblue4", xlab="Amount", ylab="Class")

```
###Kernel Density Plot
A kernel density plot is similar to a histogram, but it displays the distribution of values in a data set using one continuous curve.It is better at displaying the shape of a distribution since it isnâ€™t affected by the number of bins used in the histogram.It displays that the small amount,less than 2000 are the dense ones just like the histogram.
```{r, warning=FALSE}

#The Plot graph shows that

d <- density(train$Amount)
plot(d, main="Kernel Density Plot for Amount", xlab="Amount")
polygon(d, col="cadetblue", border="coral4")
```

###Summary of Class
The summary() function returns different values according to the given parameter, since the class is assigned as factor,the summary of the Class vector retrieve the amount of each level, that is the Boolean values namely "0" and "1". The "0" means the transaction is not fraudulent, where as "1" shows that the transaction is fraudulent. We can see that 11,952 are not frauds, only 48 transactions, which are 0.004% of 12,000 transactions are frauds.

```{r}

train$Class <- as.factor(train$Class)
summary(train[c('Class')])

```


###Scaling
The scale() function is simply standardization of the data, it is useful when you have multiple variables across different scales. Let's apply the function to the Amount Column, the data will be structured according to the specified range.We remove the Time column since it's not an essential data,let's print the head() once more,we can see here that the data is scaled and the column Time is not included since we omitted it.

```{r}

train$Amount=scale(train$Amount)
# Remove Time column
train$Time <- NULL
head(train)
```

##Data Modeling
We are going to build the linear SVM classification, the Polynomial SVM,and the Radial Kernels SVM and see the accuracy of each model.

###Linear Logistic Regression model
Summary of the model:
```{r , warning=FALSE}
glm1 <- glm(Class ~ data.matrix(Amount), data=train, family="binomial")
summary(glm1)

```
### Evaluate on the test set
The model has 95% accuracy.We have 1 TP value, that 1 fraud transactions and 3832 TN, not fraud transactions.

```{r}
probs <- predict(glm1, newdata=test, type="response")
pred <- ifelse(probs>0.5, 1, 0)
acc <- mean(pred==test$Class)
print(paste("accuracy = ", acc))
table(pred, test$Class)
```

###Linear SVM model
Let us build an SVM1 model on the train set using cost=10 and kernel="linear".

```{r}

svm1 <- svm(Class~., data=train, kernel="linear", cost=10, scale=TRUE)
summary(svm1)

```
###Evaluate the linear svm (SVM1)
Now we have a model, we can predict the value of the new data set, which is our test data set by giving inputs to our model.The table provides True positive(TP) and True negative (TN) values in the diagonal, so we have 8 TP value, that 8 fraud transactions and 3983 TN, not fraud transactions.The model has 99% accuracy.Fraud transactions are 0.002% of 3983. We remember that in the raw data, we had 0.004% of 12,000 transactions are frauds, quite similar.



```{r}

svm_pred1 <- predict(svm1, newdata=test)
table(svm_pred1, test$Class)
mean(svm_pred1==test$Class) 

```
###Linear svm Plot
We almost don't see the brown color for the frauds,since they have tiny value. The data is unbalanced data collected from two day's transactions.
Plot the Support Vectors
```{r}

plot(svm1,test, Amount~Class)

```
###Linear svm Tuning
The cost parameter determines how much  slack variables will be allowed.Experiment with various cost values to get the best model.The hyperparameters are tuned on the validation set to not over fit data and not against good principles by letting the algorithm see test data.Larger C have larger margins, smaller C, move the model toward lower bias, higher variance. The summary of tune_svm1 tells us the best cost is 10.The next syntax will use the best model value to make predictions on the test data.The least error has the 10 cost value.Best performance: 0.002506109 


```{r , warning=FALSE}
set.seed(1234)
tune_svm1 <- tune(svm, Class~., data=vald, kernel="linear",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_svm1)
best_model <- tune_svm1$best.model
summary(best_model)
pred <- predict(best_model, newdata=test)
acc_svm2 <- mean(pred==test$Class)

```

###Polynomial SVM model
Let us build an SVM2 model on the train set using cost=10 and kernel="Polynomial".

```{r}

svm2 <- svm(Class~., data=train, kernel="polynomial", cost=10, scale=TRUE)
summary(svm2)

```
###Evaluate the polynomial svm (SVM2)
Let us predict the value of the new data set with the model, SVM2, which is our test data set by giving inputs to our model.The table provides True positive(TP) and True negative (TN) values in the diagonal, so we have 12 TP value, that 12 fraud transactions and 3978 TN, not fraud transactions.The model has 99% accuracy,the same accurate as SVM linear.

```{r}

svm_pred2 <- predict(svm2, newdata=test)
table(svm_pred2, test$Class)
mean(svm_pred2==test$Class) 

```
###Polynomial svm Plot
Plot the Support Vectors
```{r}

plot(svm2,test, Amount~Class)

```
###Polynomial svm Tuning
The cost parameter determines how much  slack variables will be allowed.Experiment with various cost values to get the best model.The hyperparameters are tuned on the validation set to not over fit data and not against good principles by letting the algorithm see test data.Larger C have larger margins, smaller C, move the model toward lower bias, higher variance. The summary of tune_svm1 tells us the best cost is 0.1.The next syntax will use the best model value to make predictions on the test data.The least error has the 0.1 cost value.Best performance: 0.00174921


```{r , warning=FALSE}
set.seed(1234)
tune_svm2 <- tune(svm, Class~., data=vald, kernel="polynomial",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_svm2)
best_model <- tune_svm2$best.model
summary(best_model)
pred <- predict(best_model, newdata=test)
acc_svm3 <- mean(pred==test$Class)

```


###Radial Kernel SVM model
Let us build SVM3 model on the train set using cost=1 and kernel="radial" and predict the value of the new data set.

```{r}

svm3 <- svm(Class~., data=train, kernel="radial",
cost=1, gamma=1, scale=FALSE)
summary(svm3)


```
###Evaluate the radial kernel svm (SVM3)
Let us predict the value of the new data set with the model, SVM3, which is our test data set by giving inputs to our model.The table provides  0 TP value, that 0 fraud transactions and 3984 TN, not fraud transactions.The model has 99% accuracy, just as accurate as the polynomial svm and linear svm

```{r}
svm_pred3 <- predict(svm3, newdata=test)
table(svm_pred3, test$Class)
mean(svm_pred3==test$Class) 

```


###SVM3 Classification Plot
Plot the Support Vectors
```{r}

plot(svm3,test, Amount~Class)

```

###SVM3 Tuning
The gamma hyperparameter is tuned on validation data,larger gamma can over fit and move the model toward high variance, and lower gamma can under fit, leading the model with high bias.The summary of tune_svm3 tells us the best cost is 100 and gamma is 0.5.The next syntax will use the best model value to make predictions on the test data.The least error has the 100 cost value.Best performance: 0.003744152 

```{r, warning=FALSE}
set.seed(1234)
tune_svm3 <- tune(svm, Class~., data=vald, kernel="radial",
ranges=list(cost=c(0.1,1,10,100,1000),
gamma=c(0.5,1,2,3,4)))
summary(tune_svm3)
best_model <- tune_svm3$best.model
summary(best_model)
pred <- predict(best_model, newdata=test)
acc_svm4 <- mean(pred==test$Class)

```

